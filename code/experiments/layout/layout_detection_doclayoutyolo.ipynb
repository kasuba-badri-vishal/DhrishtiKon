{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write python code to get layout level bboxes stored from doc-layout-yolo model\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "from doclayout_yolo import YOLOv10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/model/doclayout_yolo.pt\"\n",
    "\n",
    "model = YOLOv10(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"multilingual_test_batch\"\n",
    "\n",
    "IMG_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/{NAME}/\"\n",
    "OUT_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/output/{NAME}/doclayoutyolo/\"\n",
    "\n",
    "OUT_IMG_DIR = os.path.join(OUT_DIR, \"images/\")\n",
    "OUT_JSON_DIR = os.path.join(OUT_DIR, \"json/\")\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "if not os.path.exists(OUT_IMG_DIR):\n",
    "    os.makedirs(OUT_IMG_DIR)\n",
    "if not os.path.exists(OUT_JSON_DIR):\n",
    "    os.makedirs(OUT_JSON_DIR)\n",
    "\n",
    "\n",
    "layout_map = {\n",
    "  \"0\": \"Caption\",\n",
    "  \"1\": \"Footnote\",\n",
    "  \"2\": \"Formula\",\n",
    "  \"3\": \"List-item\",\n",
    "  \"4\": \"Page-footer\",\n",
    "  \"5\": \"Page-header\",\n",
    "  \"6\": \"Picture\",\n",
    "  \"7\": \"Section-header\",\n",
    "  \"8\": \"Table\",\n",
    "  \"9\": \"Text\",\n",
    "  \"10\": \"Title\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_38.png: 1024x768 12 List-items, 4 Pictures, 2 Section-headers, 2 Texts, 90.6ms\n",
      "Speed: 3.8ms preprocess, 90.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_77.png: 1024x672 4 List-items, 4 Section-headers, 2 Tables, 3 Texts, 29.3ms\n",
      "Speed: 3.7ms preprocess, 29.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 672)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_65.png: 1024x800 1 Section-header, 9 Texts, 89.4ms\n",
      "Speed: 4.0ms preprocess, 89.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/12_217_page_1.png: 1024x736 7 List-items, 1 Section-header, 3 Texts, 29.2ms\n",
      "Speed: 5.0ms preprocess, 29.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 736)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_10.png: 1024x736 1 Caption, 1 Section-header, 1 Table, 4 Texts, 30.6ms\n",
      "Speed: 4.0ms preprocess, 30.6ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 736)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_24.png: 1024x800 2 Tables, 1 Text, 29.0ms\n",
      "Speed: 3.9ms preprocess, 29.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/77_74_page_1.png: 1024x736 2 Pictures, 1 Section-header, 2 Texts, 27.9ms\n",
      "Speed: 4.4ms preprocess, 27.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 736)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/9_955_page_1.png: 1024x640 3 List-items, 1 Text, 95.1ms\n",
      "Speed: 5.6ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 640)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/6_89_page_1.png: 1024x736 1 Page-footer, 2 Page-headers, 1 Table, 7 Texts, 31.4ms\n",
      "Speed: 6.7ms preprocess, 31.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 736)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/99_2512_page_2.png: 1024x768 (no detections), 32.9ms\n",
      "Speed: 6.4ms preprocess, 32.9ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "\n",
      "image 1/1 /data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/multilingual_test_batch/fp_56.png: 1024x736 1 Caption, 4 List-items, 1 Picture, 1 Section-header, 1 Table, 7 Texts, 27.9ms\n",
      "Speed: 3.3ms preprocess, 27.9ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 736)\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(IMG_DIR):\n",
    "    IMG_PATH = os.path.join(IMG_DIR, file)\n",
    "\n",
    "    image = Image.open(IMG_PATH)\n",
    "    \n",
    "    det_res = model.predict(\n",
    "        IMG_PATH,   # Image to predict\n",
    "        imgsz=1024,        # Prediction image size\n",
    "        conf=0.5,          # Confidence threshold\n",
    "        device=\"cuda:0\"    # Device to use (e.g., 'cuda:0' or 'cpu')\n",
    "    )\n",
    "\n",
    "    predictions = []\n",
    "    blocks = det_res[0].boxes\n",
    "\n",
    "    for block in blocks:\n",
    "        class_id = block.cls[0].item()\n",
    "        if class_id == 0:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = block.xyxy[0].tolist()\n",
    "        bbox = [x1, y1, x2, y2]\n",
    "        bbox = [round(x, 2) for x in bbox]\n",
    "        layout = layout_map[str(int(class_id))]\n",
    "        confidence = round(block.conf[0].item(), 2)\n",
    "\n",
    "        predictions.append({\n",
    "            \"layout\": layout,\n",
    "            \"bbox\": bbox,\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "        \n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle(bbox, outline='green', width=2)\n",
    "\n",
    "        # draw the layout text on the image\n",
    "        # font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        draw.text((bbox[0], bbox[1]-10), layout, fill='green')\n",
    "\n",
    "        # Save the image with bounding boxes\n",
    "        image.save(f\"{OUT_IMG_DIR}/{file}\")\n",
    "\n",
    "        # save json file\n",
    "        with open(f\"{OUT_JSON_DIR}/{file.split('.')[0]}.json\", \"w\") as f:\n",
    "            json.dump(predictions, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'layout': 'Text',\n",
       "  'bbox': [92.09, 376.54, 559.89, 450.03],\n",
       "  'confidence': 0.98},\n",
       " {'layout': 'Table',\n",
       "  'bbox': [83.95, 477.36, 573.77, 581.93],\n",
       "  'confidence': 0.97},\n",
       " {'layout': 'Text',\n",
       "  'bbox': [88.06, 626.44, 566.38, 719.63],\n",
       "  'confidence': 0.95},\n",
       " {'layout': 'Table',\n",
       "  'bbox': [78.35, 768.39, 576.31, 928.47],\n",
       "  'confidence': 0.91},\n",
       " {'layout': 'List-item',\n",
       "  'bbox': [126.4, 184.07, 543.2, 236.99],\n",
       "  'confidence': 0.81},\n",
       " {'layout': 'Section-header',\n",
       "  'bbox': [91.23, 741.84, 370.38, 755.15],\n",
       "  'confidence': 0.81},\n",
       " {'layout': 'Section-header',\n",
       "  'bbox': [92.12, 596.14, 300.9, 609.18],\n",
       "  'confidence': 0.73},\n",
       " {'layout': 'Section-header',\n",
       "  'bbox': [137.19, 84.65, 523.18, 108.53],\n",
       "  'confidence': 0.72},\n",
       " {'layout': 'Text',\n",
       "  'bbox': [262.48, 111.14, 399.01, 122.04],\n",
       "  'confidence': 0.7},\n",
       " {'layout': 'List-item',\n",
       "  'bbox': [128.22, 250.45, 452.83, 261.38],\n",
       "  'confidence': 0.56},\n",
       " {'layout': 'List-item',\n",
       "  'bbox': [151.08, 278.41, 478.59, 289.05],\n",
       "  'confidence': 0.55},\n",
       " {'layout': 'List-item',\n",
       "  'bbox': [165.79, 306.73, 489.69, 317.54],\n",
       "  'confidence': 0.54},\n",
       " {'layout': 'Section-header',\n",
       "  'bbox': [198.49, 131.34, 464.6, 170.36],\n",
       "  'confidence': 0.53}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
