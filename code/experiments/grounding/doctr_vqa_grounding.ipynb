{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import ImageDraw, Image\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ocr_predictor(det_arch='db_resnet50', reco_arch='crnn_vgg16_bn', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"BHASHINI\"\n",
    "\n",
    "MAX_MATCHES = 5\n",
    "\n",
    "CUT_OFF_THRESHOLD = 70\n",
    "QUESTION_WEIGHT = 0.2\n",
    "ANSWER_WEIGHT = 0.8\n",
    "\n",
    "LEVEL = \"line\" # or \"word_level\"\n",
    "\n",
    "IMG_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/{NAME}/\"\n",
    "\n",
    "JSON_FILE = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/input/data2.json\"\n",
    "OUT_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/output/grounding/{NAME}/doctr/{LEVEL}/\"\n",
    "\n",
    "\n",
    "IMG_DIR = \"/data/BADRI/FINAL/THESIS/GRVQA/data/CircularsVQA/BHASHINI_TESTSET/final/\"\n",
    "JSON_FILE = \"/data/BADRI/FINAL/THESIS/GRVQA/data/CircularsVQA/BHASHINI_TESTSET/final_annotations.json\"\n",
    "OUT_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/output/grounding/{NAME}/doctr/{LEVEL}/\"\n",
    "\n",
    "OUT_DET_DIR = os.path.join(OUT_DIR, \"detections\")\n",
    "OUT_IMG_DIR = os.path.join(OUT_DIR, \"images\")\n",
    "OUT_JSON_DIR = os.path.join(OUT_DIR, \"json\")\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "if not os.path.exists(OUT_IMG_DIR):\n",
    "    os.makedirs(OUT_IMG_DIR)\n",
    "if not os.path.exists(OUT_JSON_DIR):\n",
    "    os.makedirs(OUT_JSON_DIR)\n",
    "if not os.path.exists(OUT_DET_DIR):\n",
    "    os.makedirs(OUT_DET_DIR)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = {'what', 'is', 'the', 'this', 'that', 'these', 'those', 'which', 'how', 'why', 'where', 'when', 'who', 'will', 'be', 'and', 'or', 'in', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "\n",
    "# Read json data\n",
    "with open(JSON_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [15:24<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in tqdm(os.listdir(IMG_DIR)):\n",
    "    IMG_PATH = os.path.join(IMG_DIR, file)\n",
    "    doc = DocumentFile.from_images(IMG_PATH)\n",
    "    result = model(doc)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    image = Image.open(IMG_PATH)\n",
    "\n",
    "    for page in result.pages:     \n",
    "        dim = tuple(reversed(page.dimensions))\n",
    "        for block in page.blocks:\n",
    "            for line in block.lines:\n",
    "                output = {}\n",
    "                geo = line.geometry\n",
    "                a = list(a*b for a,b in zip(geo[0],dim))\n",
    "                b = list(a*b for a,b in zip(geo[1],dim))\n",
    "                x1 = round(a[0], 2).astype(float)\n",
    "                y1 = round(a[1], 2).astype(float)\n",
    "                x2 = round(b[0], 2).astype(float)\n",
    "                y2 = round(b[1], 2).astype(float)\n",
    "                line_bbox = [x1, y1, x2, y2]\n",
    "                \n",
    "                sent = []\n",
    "                words_data = []\n",
    "                for word in line.words:\n",
    "                    word_data = {}\n",
    "                    sent.append(word.value)\n",
    "                    geo = word.geometry\n",
    "                    a = list(a*b for a,b in zip(geo[0],dim))\n",
    "                    b = list(a*b for a,b in zip(geo[1],dim))\n",
    "                    x1 = round(a[0], 2).astype(float)\n",
    "                    y1 = round(a[1], 2).astype(float)\n",
    "                    x2 = round(b[0], 2).astype(float)\n",
    "                    y2 = round(b[1], 2).astype(float)\n",
    "                    bbox = [x1, y1, x2, y2]\n",
    "                    \n",
    "                    word_data['bbox'] = bbox\n",
    "                    word_data['text'] = word.value\n",
    "                    words_data.append(word_data)\n",
    "                output['bbox'] = line_bbox\n",
    "                output['text'] = \" \".join(sent)\n",
    "                output['words'] = words_data\n",
    "                predictions.append(output)\n",
    "\n",
    "                # Draw bounding boxes on the image\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.rectangle(line_bbox, outline='red', width=2)\n",
    "\n",
    "\n",
    "    # save predictions in json\n",
    "    with open(os.path.join(OUT_JSON_DIR, f\"{file}.json\"), \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "\n",
    "    # save image\n",
    "    image.save(os.path.join(OUT_DET_DIR, f\"{file}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matched_regions(question_text, target_text, predictions):\n",
    "\n",
    "    question_terms = [word.lower() for word in question_text.split() if word.lower() not in stop_words]\n",
    "    matched_regions = []\n",
    "    for region in predictions:\n",
    "        region_text = region['text']\n",
    "        region_copy = region.copy()\n",
    "\n",
    "        if target_text.lower() in region_text.lower():\n",
    "            region_copy['match_score'] = 100\n",
    "            region_copy['match_details'] = {\n",
    "                    'exact_match': True,\n",
    "                    'answer_score': 100,\n",
    "                    'question_score': 100\n",
    "                }\n",
    "            matched_regions.append(region_copy)\n",
    "            continue\n",
    "\n",
    "        partial_score = fuzz.partial_ratio(target_text.lower(), region_text.lower())\n",
    "        token_score = fuzz.token_set_ratio(target_text.lower(), region_text.lower())\n",
    "        \n",
    "        # Calculate length factor (preference for longer matches that contain meaningful content)\n",
    "        target_len = len(target_text)\n",
    "        region_len = len(region_text)\n",
    "        length_factor = min(1.0, region_len / min(50, target_len))  # Cap at 1.0, adapt based on target length\n",
    "        \n",
    "        # Combine scores for answer with weights\n",
    "        # Higher weight to token matching for longer texts, higher weight to partial matching for shorter texts\n",
    "        if region_len > 10:\n",
    "            answer_score = (partial_score * 0.3) + (token_score * 0.5) + (length_factor * 100 * 0.2)\n",
    "        else:\n",
    "            # For very short texts, reduce their overall score unless they're exact matches\n",
    "            answer_score = (partial_score * 0.3) + (token_score * 0.4) + (length_factor * 100 * 0.3)\n",
    "            if region_len < 5 and partial_score < 100:\n",
    "                answer_score *= 0.5  # Penalize very short inexact matches\n",
    "\n",
    "        # penalize shorter region_texts\n",
    "        if region_len < 5:\n",
    "            answer_score *= 0.5\n",
    "        \n",
    "        # Calculate fuzzy match scores for question terms using both methods\n",
    "        partial_question_scores = [fuzz.partial_ratio(term, region_text.lower()) for term in question_terms]\n",
    "        token_question_scores = [fuzz.token_set_ratio(term, region_text.lower()) for term in question_terms]\n",
    "        \n",
    "        # Get best scores for question terms\n",
    "        best_partial_question = max(partial_question_scores) if partial_question_scores else 0\n",
    "        best_token_question = max(token_question_scores) if token_question_scores else 0\n",
    "        \n",
    "        # Combine question scores\n",
    "        question_score = (best_partial_question * 0.4) + (best_token_question * 0.6)\n",
    "        \n",
    "        # Combine scores (giving more weight to answer matches)\n",
    "        combined_score = (answer_score * ANSWER_WEIGHT) + (question_score * QUESTION_WEIGHT)\n",
    "\n",
    "        # print(combined_score)\n",
    "        \n",
    "        if combined_score >= CUT_OFF_THRESHOLD:\n",
    "            region_copy['match_score'] = combined_score\n",
    "            region_copy['match_details'] = {\n",
    "                'exact_match': False,\n",
    "                'answer_score': answer_score,\n",
    "                'question_score': question_score,\n",
    "                'answer_weight': ANSWER_WEIGHT,\n",
    "                'question_weight': QUESTION_WEIGHT\n",
    "            }\n",
    "            matched_regions.append(region_copy)\n",
    "\n",
    "\n",
    "    matched_regions.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "    top_matches = matched_regions[:MAX_MATCHES]\n",
    "    return top_matches\n",
    "        \n",
    "        \n",
    "def longest_consecutive_range(indices):\n",
    "    if not indices:\n",
    "        return []\n",
    "\n",
    "    indices = sorted(set(indices))\n",
    "    longest = []\n",
    "    current = [indices[0]]\n",
    "\n",
    "    for i in range(1, len(indices)):\n",
    "        if indices[i] == indices[i - 1] + 1:\n",
    "            current.append(indices[i])\n",
    "        else:\n",
    "            if len(current) > len(longest):\n",
    "                longest = current\n",
    "            current = [indices[i]]\n",
    "\n",
    "    if len(current) > len(longest):\n",
    "        longest = current\n",
    "\n",
    "    return longest\n",
    "\n",
    "\n",
    "def get_word_level_matches(answer_text, top_k_matches):\n",
    "    bboxes = []\n",
    "    for match in top_k_matches:\n",
    "        indices = []\n",
    "        for index, word in enumerate(match['words']):\n",
    "            if word['text'].lower() in answer_text.lower():\n",
    "                # bboxes.append(word['bbox'])\n",
    "                indices.append(index)\n",
    "        longest_indices = longest_consecutive_range(indices)\n",
    "        for index in longest_indices:\n",
    "            bboxes.append(match['words'][index]['bbox'])\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 502/502 [19:18<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "for image_name, qna_pairs in tqdm(data.items()):\n",
    "    IMG_PATH = os.path.join(IMG_DIR, image_name)\n",
    "    \n",
    "\n",
    "    json_file = os.path.join(OUT_JSON_DIR, f\"{image_name}.json\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    # print(predictions)\n",
    "\n",
    "    qna_count = 0\n",
    "    for qna_pair in qna_pairs:\n",
    "        image = Image.open(IMG_PATH)\n",
    "\n",
    "        question_text = qna_pair['question']\n",
    "        answer_text = qna_pair['answer']\n",
    "        \n",
    "        top_k_matches = get_matched_regions(question_text, answer_text, predictions)\n",
    "\n",
    "        # if answer_text == \"Recruitment Notification No.1/2021.\":\n",
    "        #     print(top_k_matches)\n",
    "\n",
    "        #     check = top_k_matches\n",
    "        #     break\n",
    "\n",
    "        if LEVEL == \"word\":\n",
    "\n",
    "            word_level_matches = get_word_level_matches(answer_text, top_k_matches)\n",
    "            for bbox in word_level_matches:\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.rectangle(bbox, outline='green', width=2)\n",
    "\n",
    "        else :\n",
    "            for match in top_k_matches:\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.rectangle(match['bbox'], outline='blue', width=2)\n",
    "\n",
    "        # # write qna pair in the image\n",
    "        draw.text((10, 10), \"Question:\" + question_text, fill='red')\n",
    "        draw.text((10, 25), \"Answer: \"+answer_text, fill='red')\n",
    "\n",
    "        image.save(os.path.join(OUT_IMG_DIR, f\"{image_name}_{qna_count}.png\"))\n",
    "        qna_count += 1\n",
    "\n",
    "        # break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
