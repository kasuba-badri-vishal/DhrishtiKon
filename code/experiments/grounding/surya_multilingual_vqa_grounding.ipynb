{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/venv/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageDraw, Image\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from PIL import Image\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_predictor = RecognitionPredictor()\n",
    "detection_predictor = DetectionPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"telugu\"\n",
    "\n",
    "MAX_MATCHES = 5\n",
    "\n",
    "CUT_OFF_THRESHOLD = 70\n",
    "QUESTION_WEIGHT = 0.2\n",
    "ANSWER_WEIGHT = 0.8\n",
    "\n",
    "DATA_DIR = \"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/\"\n",
    "\n",
    "LEVEL = \"line\" # or \"word_level\"\n",
    "\n",
    "IMG_DIR = f\"{DATA_DIR}/input/{NAME}/\"\n",
    "JSON_FILE = f\"{DATA_DIR}/input/data2.json\"\n",
    "OUT_DIR = f\"{DATA_DIR}/output/grounding/{NAME}/surya/{LEVEL}/\"\n",
    "\n",
    "LANGS = [\"en\", \"te\"]\n",
    "\n",
    "\n",
    "# IMG_DIR = \"/data/BADRI/FINAL/THESIS/GRVQA/data/CircularsVQA/BHASHINI_TESTSET/final/\"\n",
    "# JSON_FILE = \"/data/BADRI/FINAL/THESIS/GRVQA/data/CircularsVQA/BHASHINI_TESTSET/final_annotations.json\"\n",
    "# OUT_DIR = f\"/data/BADRI/FINAL/THESIS/GRVQA/gr-doc-vqa-grounding/data/output/grounding/{NAME}/doctr/{LEVEL}/\"\n",
    "\n",
    "OUT_DET_DIR = os.path.join(OUT_DIR, \"detections\")\n",
    "OUT_IMG_DIR = os.path.join(OUT_DIR, \"images\")\n",
    "OUT_JSON_DIR = os.path.join(OUT_DIR, \"json\")\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "if not os.path.exists(OUT_IMG_DIR):\n",
    "    os.makedirs(OUT_IMG_DIR)\n",
    "if not os.path.exists(OUT_JSON_DIR):\n",
    "    os.makedirs(OUT_JSON_DIR)\n",
    "if not os.path.exists(OUT_DET_DIR):\n",
    "    os.makedirs(OUT_DET_DIR)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = {'what', 'is', 'the', 'this', 'that', 'these', 'those', 'which', 'how', 'why', 'where', 'when', 'who', 'will', 'be', 'and', 'or', 'in', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "\n",
    "# # Read json data\n",
    "# with open(JSON_FILE, \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surya_predictions(img_path = None, langs = [\"en\"]):\n",
    "    image = Image.open(img_path)\n",
    "    preds = recognition_predictor([image], [langs], detection_predictor)\n",
    "\n",
    "    predictions = []\n",
    "    for pred in preds[0].text_lines:\n",
    "        bbox = pred.bbox\n",
    "        text = pred.text\n",
    "        predictions.append({\"bbox\":bbox,\"text\":text})\n",
    "\n",
    "        # Draw bounding boxes on the image\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle(bbox, outline='red', width=2)\n",
    "\n",
    "    \n",
    "\n",
    "    return predictions, image\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def longest_consecutive_range(indices):\n",
    "    if not indices:\n",
    "        return []\n",
    "\n",
    "    indices = sorted(set(indices))\n",
    "    longest = []\n",
    "    current = [indices[0]]\n",
    "\n",
    "    for i in range(1, len(indices)):\n",
    "        if indices[i] == indices[i - 1] + 1:\n",
    "            current.append(indices[i])\n",
    "        else:\n",
    "            if len(current) > len(longest):\n",
    "                longest = current\n",
    "            current = [indices[i]]\n",
    "\n",
    "    if len(current) > len(longest):\n",
    "        longest = current\n",
    "\n",
    "    return longest\n",
    "\n",
    "\n",
    "def get_word_level_matches(answer_text, top_k_matches):\n",
    "    bboxes = []\n",
    "    for match in top_k_matches:\n",
    "        indices = []\n",
    "        for index, word in enumerate(match['words']):\n",
    "            if word['text'].lower() in answer_text.lower():\n",
    "                # bboxes.append(word['bbox'])\n",
    "                indices.append(index)\n",
    "        longest_indices = longest_consecutive_range(indices)\n",
    "        for index in longest_indices:\n",
    "            bboxes.append(match['words'][index]['bbox'])\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "  8%|▊         | 1/13 [00:04<00:50,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      " 15%|█▌        | 2/13 [00:07<00:40,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      " 23%|██▎       | 3/13 [00:10<00:31,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      " 31%|███       | 4/13 [00:13<00:28,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      " 38%|███▊      | 5/13 [00:15<00:23,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      " 46%|████▌     | 6/13 [00:18<00:20,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      " 54%|█████▍    | 7/13 [00:22<00:19,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      " 62%|██████▏   | 8/13 [00:25<00:15,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      " 69%|██████▉   | 9/13 [00:28<00:12,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      " 77%|███████▋  | 10/13 [00:31<00:09,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      " 85%|████████▍ | 11/13 [00:34<00:06,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      " 92%|█████████▏| 12/13 [00:38<00:03,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recognition model s3://text_recognition/2025_02_18 on device cuda with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device cuda with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 13/13 [00:41<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir(IMG_DIR)):\n",
    "    IMG_PATH = os.path.join(IMG_DIR, file)\n",
    "    predictions, image = get_surya_predictions(IMG_PATH, LANGS)\n",
    "\n",
    "     # save predictions in json\n",
    "    with open(os.path.join(OUT_JSON_DIR, f\"{file}.json\"), \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # save image\n",
    "    image.save(os.path.join(OUT_DET_DIR, f\"{file}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, qna_pairs in tqdm(data.items()):\n",
    "    IMG_PATH = os.path.join(IMG_DIR, image_name)\n",
    "    \n",
    "\n",
    "    json_file = os.path.join(OUT_JSON_DIR, f\"{image_name}.json\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    # print(predictions)\n",
    "\n",
    "    qna_count = 0\n",
    "    for qna_pair in qna_pairs:\n",
    "        image = Image.open(IMG_PATH)\n",
    "\n",
    "        question_text = qna_pair['question']\n",
    "        answer_text = qna_pair['answer']\n",
    "        \n",
    "        top_k_matches = get_matched_regions(question_text, answer_text, predictions)\n",
    "\n",
    "\n",
    "        if LEVEL == \"word\":\n",
    "\n",
    "            word_level_matches = get_word_level_matches(answer_text, top_k_matches)\n",
    "            for bbox in word_level_matches:\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.rectangle(bbox, outline='green', width=2)\n",
    "\n",
    "        else :\n",
    "            for match in top_k_matches:\n",
    "                draw = ImageDraw.Draw(image)\n",
    "                draw.rectangle(match['bbox'], outline='blue', width=2)\n",
    "\n",
    "        # # write qna pair in the image\n",
    "        draw.text((10, 10), \"Question:\" + question_text, fill='red')\n",
    "        draw.text((10, 25), \"Answer: \"+answer_text, fill='red')\n",
    "\n",
    "        image.save(os.path.join(OUT_IMG_DIR, f\"{image_name}_{qna_count}.png\"))\n",
    "        qna_count += 1\n",
    "\n",
    "        # break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
